import sys
import os
import json
import time
import jwt
import requests
import logging
import hashlib
from io import BytesIO
from datetime import datetime
from typing import List, Optional, Dict
from enum import Enum
from functools import lru_cache

from fastapi import FastAPI, UploadFile, File, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from sqlalchemy.orm import Session
from sqlalchemy import func
# –ó–∞–≥—Ä—É–∂–∞–µ–º .env
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ==================== –ë–ê–ó–ê –î–ê–ù–ù–´–• ====================

from database import get_db, AnalysisHistory, init_db
from sqlalchemy import desc

# ==================== –ö–≠–®–ò–†–û–í–ê–ù–ò–ï ====================

@lru_cache(maxsize=50)
def get_text_hash(text: str) -> str:
    """–•—ç—à –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    return hashlib.md5(text[:2000].encode()).hexdigest()

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
_analysis_cache: Dict[str, 'AnalysisResult'] = {}

# ==================== –ú–û–î–ï–õ–ò ====================

class DocumentType(str, Enum):
    contract = "contract"
    invoice = "invoice"
    act = "act"
    application = "application"  # ‚Üê –î–æ–±–∞–≤–∏–ª–∏!
    other = "other"

class RiskLevel(str, Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"

class RiskFlag(BaseModel):
    level: RiskLevel
    category: str
    description: str
    suggestion: str

class ExtractedData(BaseModel):
    document_type: DocumentType
    parties: List[str] = Field(default_factory=list)
    total_amount: Optional[float] = None
    currency: Optional[str] = "RUB"
    dates: Dict[str, Optional[str]] = Field(default_factory=dict)
    obligations: List[str] = Field(default_factory=list)
    penalties: Optional[str] = None

class AnalysisResult(BaseModel):
    extracted_data: ExtractedData
    risk_flags: List[RiskFlag] = Field(default_factory=list)
    action_items: List[str] = Field(default_factory=list)
    summary: str
    confidence_score: float = Field(ge=0, le=1)

class DocumentUploadResponse(BaseModel):
    status: str
    result: Optional[AnalysisResult] = None
    error: Optional[str] = None

# ==================== YANDEX GPT SERVICE ====================

class YandexGPTService:
    def __init__(self, folder_id: str, key_path: str = None):
        self.folder_id = folder_id
        self.iam_token = None
        self.token_expires_at = 0
        
        # üîë –ß–∏—Ç–∞–µ–º –∫–ª—é—á –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        key_content = os.getenv('AUTHORIZED_KEY_CONTENT')
        if key_content:
            self.key_data = json.loads(key_content)
            logger.info("‚úÖ –ö–ª—é—á –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è")
        elif key_path and os.path.exists(key_path):
            # –§–æ–ª–±—ç–∫: —Ñ–∞–π–ª (–¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏)
            with open(key_path, 'r', encoding='utf-8') as f:
                self.key_data = json.load(f)
            logger.info(f"‚úÖ –ö–ª—é—á –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ —Ñ–∞–π–ª–∞")
        else:
            raise RuntimeError("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω –∫–ª—é—á Yandex GPT! –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ AUTHORIZED_KEY_CONTENT")
        
        self.service_account_id = self.key_data['service_account_id']
        self.private_key = self.key_data['private_key']
        self.key_id = self.key_data['id']
    
    def get_iam_token(self) -> str:
        now = time.time()
        if self.iam_token and now < self.token_expires_at:
            return self.iam_token
        
        payload = {
            'aud': "https://iam.api.cloud.yandex.net/iam/v1/tokens",
            'iss': self.service_account_id,
            'iat': int(now),
            'exp': int(now) + 3600
        }
        
        headers = {'kid': self.key_id, 'alg': 'PS256', 'typ': 'JWT'}
        encoded_token = jwt.encode(payload, self.private_key, algorithm='PS256', headers=headers)
        
        resp = requests.post(
            "https://iam.api.cloud.yandex.net/iam/v1/tokens",
            headers={"Content-Type": "application/json"},
            json={"jwt": encoded_token}
        )
        
        if resp.status_code != 200:
            raise Exception(f"Failed to get IAM token: {resp.text}")
        
        self.iam_token = resp.json()["iamToken"]
        self.token_expires_at = now + 3600
        return self.iam_token
    
    def call_gpt(self, prompt: str, max_tokens: int = 1200) -> str:
        iam_token = self.get_iam_token()
        url = "https://llm.api.cloud.yandex.net/foundationModels/v1/completion"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {iam_token}",
            "x-folder-id": self.folder_id
        }
        data = {
            "modelUri": f"gpt://{self.folder_id}/yandexgpt-lite",
            "completionOptions": {
                "stream": False,
                "temperature": 0.1,
                "maxTokens": max_tokens,
                "preset": "balanced"
            },
            "messages": [{"role": "user", "text": prompt}]
        }
        response = requests.post(url, headers=headers, json=data)
        if response.status_code != 200:
            raise Exception(f"GPT error: {response.text}")
        return response.json()['result']['alternatives'][0]['message']['text']

# ==================== DOCUMENT AGENT ====================

class DocumentAgent:
    def __init__(self, gpt_service: YandexGPTService):
        self.gpt = gpt_service
    
    def extract_text_from_pdf(self, file_content: bytes) -> str:
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º"""
        try:
            from PyPDF2 import PdfReader
            reader = PdfReader(BytesIO(file_content))
            
            text = ""
            for page in reader.pages[:10]:
                extracted = page.extract_text()
                if extracted:
                    text += extracted + "\n"
                if len(text) > 5000:
                    break
            
            return text.strip()
        except Exception as e:
            logger.error(f"PDF parse error: {e}")
            return "[–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF]"
    
    def analyze_document(self, text: str) -> AnalysisResult:
        text_hash = get_text_hash(text)
        if text_hash in _analysis_cache:
            logger.info("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∑—è—Ç –∏–∑ –∫—ç—à–∞")
            return _analysis_cache[text_hash]
        
        combined_prompt = f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –∏–∑–≤–ª–µ–∫–∏ –í–°–ï –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON.

üìÑ –¢–ï–ö–°–¢ –î–û–ö–£–ú–ï–ù–¢–ê:
{text[:4000]}

üìã –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (—Å—Ç—Ä–æ–≥–æ JSON):
{{
  "extracted_data": {{
    "document_type": "contract|invoice|act|application|other",
    "document_subtype": "loan|rental|service|purchase|microloan_application|other",
    "parties": ["–°—Ç–æ—Ä–æ–Ω–∞ 1", "–°—Ç–æ—Ä–æ–Ω–∞ 2"],
    "total_amount": 5800,
    "currency": "RUB",
    "dates": {{
      "signature": "2024-01-01",
      "start_date": "2024-01-01",
      "end_date": "2024-12-31",
      "payment_due": "2024-01-15"
    }},
    "financial_terms": {{
      "interest_rate": "0.8% –≤ –¥–µ–Ω—å (292% –≥–æ–¥–æ–≤—ã—Ö)",
      "loan_term": "30 –¥–Ω–µ–π",
      "monthly_payment": 11000,
      "penalties": "10% –æ—Ç —Å—É–º–º—ã –ø—Ä–æ—Å—Ä–æ—á–∫–∏",
      "payment_schedule": "–µ–¥–∏–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ"
    }},
    "rental_terms": {{
      "monthly_rent": 50000,
      "deposit": 50000,
      "utilities": "–∞—Ä–µ–Ω–¥–∞—Ç–æ—Ä –ø–ª–∞—Ç–∏—Ç –æ—Ç–¥–µ–ª—å–Ω–æ",
      "lease_duration": "11 –º–µ—Å—è—Ü–µ–≤"
    }},
    "applicant_info": {{
      "full_name": "–ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤–∏—á",
      "birth_date": "1990-01-01",
      "passport": "1234 567890",
      "inn": "123456789012",
      "snils": "12345678901",
      "phone": "+79991234567",
      "email": "email@example.com",
      "monthly_income": 80000,
      "employment": "–Ω–∞–µ–º–Ω—ã–π —Å–æ—Ç—Ä—É–¥–Ω–∏–∫",
      "marital_status": "—Ä–∞–∑–≤–µ–¥–µ–Ω(–∞)",
      "children_count": 1
    }},
    "items": ["—Ç–æ–≤–∞—Ä/—É—Å–ª—É–≥–∞ 1", "—Ç–æ–≤–∞—Ä/—É—Å–ª—É–≥–∞ 2"],
    "obligations": ["–æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ 1"],
    "penalties": "–æ–ø–∏—Å–∞–Ω–∏–µ —à—Ç—Ä–∞—Ñ–æ–≤",
    "requisites": {{
      "inn": "...",
      "bank_account": "..."
    }}
  }},
  "risk_flags": [
    {{"level": "high|medium|low", "category": "financial|legal|operational", "description": "...", "suggestion": "..."}}
  ],
  "action_items": ["–¥–µ–π—Å—Ç–≤–∏–µ 1", "–¥–µ–π—Å—Ç–≤–∏–µ 2"],
  "summary": "–ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è",
  "confidence_score": 0.85
}}

‚ö†Ô∏è –ü–†–ê–í–ò–õ–ê:
‚Ä¢ parties ‚Äî –°–ü–ò–°–û–ö —Å—Ç—Ä–æ–∫: ["–û–û–û –í–≠–ë–ë–ê–ù–ö–ò–†", "–ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤–∏—á"]
‚Ä¢ document_subtype –æ–ø—Ä–µ–¥–µ–ª–∏ —Ç–æ—á–Ω–æ:
  - microloan_application = –∑–∞—è–≤–ª–µ–Ω–∏–µ –Ω–∞ –º–∏–∫—Ä–æ–∑–∞–π–º (–∑–∞–ø–æ–ª–Ω–∏ applicant_info)
  - loan = –¥–æ–≥–æ–≤–æ—Ä –∑–∞–π–º–∞/–∫—Ä–µ–¥–∏—Ç–∞ (–∑–∞–ø–æ–ª–Ω–∏ financial_terms)
  - rental = –∞—Ä–µ–Ω–¥–∞ (–∑–∞–ø–æ–ª–Ω–∏ rental_terms)
  - invoice = —Å—á—ë—Ç (—É–∫–∞–∂–∏ items –∏ payment_due)
  - act = –∞–∫—Ç (—É–∫–∞–∂–∏ items)
‚Ä¢ –ò–ó–í–õ–ï–ö–ê–ô –í–°–Å —á—Ç–æ –µ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ:
  - –î–ª—è –∑–∞—è–≤–ª–µ–Ω–∏–π: –ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –ø–∞—Å–ø–æ—Ä—Ç, –ò–ù–ù, –°–ù–ò–õ–°, –¥–æ—Ö–æ–¥, –∫–æ–Ω—Ç–∞–∫—Ç—ã
  - –î–ª—è –¥–æ–≥–æ–≤–æ—Ä–æ–≤: —Å—É–º–º—ã, —Å—Ä–æ–∫–∏, –ø—Ä–æ—Ü–µ–Ω—Ç—ã, —à—Ç—Ä–∞—Ñ—ã, —É—Å–ª–æ–≤–∏—è
  - –î–ª—è —Å—á–µ—Ç–æ–≤: —Ç–æ–≤–∞—Ä—ã, —Å—É–º–º—ã, —Å—Ä–æ–∫–∏ –æ–ø–ª–∞—Ç—ã
‚Ä¢ –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äî —Å—Ç–∞–≤—å null
‚Ä¢ –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON, –±–µ–∑ markdown
‚Ä¢ confidence_score: 0.0-1.0

üéØ –û–°–û–ë–û–ï –í–ù–ò–ú–ê–ù–ò–ï:
‚Ä¢ –î–ª—è –º–∏–∫—Ä–æ–∑–∞–π–º–æ–≤: –∏–∑–≤–ª–µ–∫–∏ –í–°–ï –ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–§–ò–û, –ø–∞—Å–ø–æ—Ä—Ç, –ò–ù–ù, –°–ù–ò–õ–°, —Ç–µ–ª–µ—Ñ–æ–Ω, email, –¥–æ—Ö–æ–¥)
‚Ä¢ –î–ª—è –∞—Ä–µ–Ω–¥—ã: —Å—É–º–º–∞ –∞—Ä–µ–Ω–¥—ã, –∑–∞–ª–æ–≥, —Å—Ä–æ–∫, –∫–æ–º–º—É–Ω–∞–ª–∫–∞, —à—Ç—Ä–∞—Ñ—ã –∑–∞ –≤—ã–µ–∑–¥
‚Ä¢ –î–ª—è –∫—Ä–µ–¥–∏—Ç–æ–≤: –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è —Å—Ç–∞–≤–∫–∞ (–≤ –¥–µ–Ω—å –∏ –≥–æ–¥–æ–≤—ã—Ö), —Å—Ä–æ–∫, –µ–∂–µ–º–µ—Å—è—á–Ω—ã–π –ø–ª–∞—Ç—ë–∂
‚Ä¢ –ò–©–ò —Ä–∏—Å–∫–∏: –≤—ã—Å–æ–∫–∏–µ –ø—Ä–æ—Ü–µ–Ω—Ç—ã, —Å–∫—Ä—ã—Ç—ã–µ –∫–æ–º–∏—Å—Å–∏–∏, –æ–¥–Ω–æ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–µ —É—Å–ª–æ–≤–∏—è
"""
        
        response = self.gpt.call_gpt(combined_prompt, max_tokens=1200)
        
        try:
            start = response.find('{')
            end = response.rfind('}') + 1
            data = json.loads(response[start:end])
        except Exception as e:
            logger.warning(f"JSON parse error: {e}")
            data = {
                "extracted_data": {
                    "document_type": "other",
                    "parties": [],
                    "total_amount": None,
                    "currency": "–ù–µ —É–∫–∞–∑–∞–Ω–∞",
                    "dates": {},
                    "obligations": [],
                    "penalties": None
                },
                "risk_flags": [],
                "action_items": ["–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –≤—Ä—É—á–Ω—É—é"],
                "summary": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç",
                "confidence_score": 0.3
            }
        
        ext = data.get("extracted_data", {})
        result = AnalysisResult(
            extracted_data=ExtractedData(
                document_type=DocumentType(ext.get("document_type", "other")),
                parties=ext.get("parties") or [],
                total_amount=ext.get("total_amount"),
                currency=ext.get("currency") or "–ù–µ —É–∫–∞–∑–∞–Ω–∞",
                dates=ext.get("dates") or {},
                obligations=ext.get("obligations") or [],
                penalties=ext.get("penalties")
            ),
            risk_flags=[
                RiskFlag(
                    level=RiskLevel(f.get("level", "low")),
                    category=f.get("category", "other"),
                    description=f.get("description", ""),
                    suggestion=f.get("suggestion", "")
                ) for f in (data.get("risk_flags") or [])
            ],
            action_items=data.get("action_items") or ["–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Ä—É—á–Ω—É—é"],
            summary=data.get("summary", ""),
            confidence_score=min(1.0, max(0.0, data.get("confidence_score", 0.5)))
        )
        
        _analysis_cache[text_hash] = result
        logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ –∫—ç—à (–≤—Å–µ–≥–æ: {len(_analysis_cache)})")
        
        return result

# ==================== FASTAPI APP ====================

app = FastAPI(title="DocuBot API", description="AI-–∞–≥–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", version="0.2.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Å–µ—Ä–≤–∏—Å–æ–≤
init_db()
logger.info("‚úÖ Database initialized")

FOLDER_ID = os.getenv("YANDEX_FOLDER_ID", "b1gdcuaq0il54iojm93b")
gpt_service = YandexGPTService(FOLDER_ID)  # KEY_PATH –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–µ–Ω!
agent = DocumentAgent(gpt_service)

@app.get("/")
async def root():
    return {"message": "DocuBot API —Ä–∞–±–æ—Ç–∞–µ—Ç!", "version": "0.2.0"}

@app.get("/health")
async def health_check():
    return {"status": "ok"}

@app.get("/cache/stats")
async def cache_stats():
    return {
        "cache_size": len(_analysis_cache),
        "cache_info": get_text_hash.cache_info()
    }

@app.post("/api/analyze", response_model=DocumentUploadResponse)
async def analyze_document(file: UploadFile = File(...), db: Session = Depends(get_db)):
    logger.info(f"–ü–æ–ª—É—á–µ–Ω —Ñ–∞–π–ª: {file.filename}")
    try:
        content = await file.read()
        text = agent.extract_text_from_pdf(content)
        if not text or len(text) < 10:
            raise HTTPException(400, "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç")
        
        result = agent.analyze_document(text)
        
        # üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
        try:
            history = AnalysisHistory(
                filename=file.filename,
                document_type=result.extracted_data.document_type.value,
                parties=str(result.extracted_data.parties),
                total_amount=result.extracted_data.total_amount,
                currency=result.extracted_data.currency,
                summary=result.summary,
                confidence_score=result.confidence_score,
                risk_count=len(result.risk_flags),
                full_result=result.dict(),
                user_id="web"
            )
            db.add(history)
            db.commit()
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î: {e}")
            db.rollback()
        
        return DocumentUploadResponse(status="success", result=result)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞: {str(e)}")
        return DocumentUploadResponse(status="error", error=str(e))

@app.get("/api/history")
async def get_history(limit: int = 10, skip: int = 0, db: Session = Depends(get_db)):
    """–ü–æ–ª—É—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –∞–Ω–∞–ª–∏–∑–æ–≤"""
    try:
        analyses = db.query(AnalysisHistory).order_by(
            desc(AnalysisHistory.created_at)
        ).offset(skip).limit(limit).all()
        
        return {
            "status": "success",
            "count": len(analyses),
            "analyses": [
                {
                    "id": a.id,
                    "filename": a.filename,
                    "document_type": a.document_type,
                    "created_at": a.created_at.isoformat(),
                    "confidence_score": a.confidence_score,
                    "risk_count": a.risk_count
                }
                for a in analyses
            ]
        }
    except Exception as e:
        logger.error(f"Error fetching history: {e}")
        return {"status": "error", "error": str(e)}

@app.get("/api/stats")
async def get_stats(db: Session = Depends(get_db)):
    """–ü–æ–ª—É—á–∏—Ç—å –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
    try:
        # –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        total_documents = db.query(AnalysisHistory).count()
        
        # –î–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —Ç–∏–ø–∞–º
        contracts = db.query(AnalysisHistory).filter(
            AnalysisHistory.document_type == "contract"
        ).count()
        
        invoices = db.query(AnalysisHistory).filter(
            AnalysisHistory.document_type == "invoice"
        ).count()
        
        acts = db.query(AnalysisHistory).filter(
            AnalysisHistory.document_type == "act"
        ).count()
        
        # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        avg_confidence = db.query(
          func.avg(AnalysisHistory.confidence_score)
        ).scalar() or 0
        
        # –í—Å–µ–≥–æ —Ä–∏—Å–∫–æ–≤
        total_risks = db.query(
            func.sum(AnalysisHistory.risk_count)
        ).scalar() or 0
        
        return {
            "status": "success",
            "total_documents": total_documents,
            "by_type": {
                "contract": contracts,
                "invoice": invoices,
                "act": acts,
                "other": total_documents - contracts - invoices - acts
            },
            "avg_confidence": round(avg_confidence, 2),
            "total_risks": total_risks
        }
    except Exception as e:
        logger.error(f"Error fetching stats: {e}")
        return {"status": "error", "error": str(e)}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=int(os.getenv("PORT", 10000)))