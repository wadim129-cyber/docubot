import sys
import os
import json
import time
import jwt
import requests
import logging
import hashlib
from io import BytesIO
from datetime import datetime
from typing import List, Optional, Dict
from enum import Enum
from functools import lru_cache

from fastapi import FastAPI, UploadFile, File, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from sqlalchemy.orm import Session
from sqlalchemy import func

# –î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ PDF
from reportlab.lib.pagesizes import A4
from reportlab.pdfgen import canvas
from reportlab.lib.units import mm

# –ó–∞–≥—Ä—É–∂–∞–µ–º .env
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ==================== –ë–ê–ó–ê –î–ê–ù–ù–´–• ====================

from database import get_db, AnalysisHistory, init_db
from sqlalchemy import desc

# ==================== –ö–≠–®–ò–†–û–í–ê–ù–ò–ï ====================

@lru_cache(maxsize=50)
def get_text_hash(text: str) -> str:
    """–•—ç—à –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    return hashlib.md5(text[:2000].encode()).hexdigest()

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
_analysis_cache: Dict[str, 'AnalysisResult'] = {}

# ==================== –ú–û–î–ï–õ–ò ====================

class DocumentType(str, Enum):
    contract = "contract"
    invoice = "invoice"
    act = "act"
    application = "application"
    other = "other"

class RiskLevel(str, Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"

class RiskFlag(BaseModel):
    level: RiskLevel
    category: str
    description: str
    suggestion: str

class ExtractedData(BaseModel):
    document_type: DocumentType
    parties: List[str] = Field(default_factory=list)
    total_amount: Optional[float] = None
    currency: Optional[str] = "RUB"
    dates: Dict[str, Optional[str]] = Field(default_factory=dict)
    obligations: List[str] = Field(default_factory=list)
    penalties: Optional[str] = None

class AnalysisResult(BaseModel):
    extracted_data: ExtractedData
    risk_flags: List[RiskFlag] = Field(default_factory=list)
    action_items: List[str] = Field(default_factory=list)
    summary: str
    confidence_score: float = Field(ge=0, le=1)

class DocumentUploadResponse(BaseModel):
    status: str
    result: Optional[AnalysisResult] = None
    error: Optional[str] = None

# ==================== YANDEX GPT SERVICE ====================

class YandexGPTService:
    def __init__(self, folder_id: str, key_path: str = None):
        self.folder_id = folder_id
        self.iam_token = None
        self.token_expires_at = 0
        
        # üîë –ß–∏—Ç–∞–µ–º –∫–ª—é—á –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        key_content = os.getenv('AUTHORIZED_KEY_CONTENT')
        if key_content:
            self.key_data = json.loads(key_content)
            logger.info("‚úÖ –ö–ª—é—á –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è")
        # üîë –ß–∏—Ç–∞–µ–º –∏–∑ —Ñ–∞–π–ª–∞ authorized_key.json (–ª–æ–∫–∞–ª—å–Ω–æ)
        elif os.path.exists('authorized_key.json'):
            with open('authorized_key.json', 'r', encoding='utf-8') as f:
                self.key_data = json.load(f)
            logger.info("‚úÖ –ö–ª—é—á –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ —Ñ–∞–π–ª–∞ authorized_key.json")
        # üîë –§–æ–ª–±—ç–∫: —Ñ–∞–π–ª –ø–æ –ø—É—Ç–∏ (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
        elif key_path and os.path.exists(key_path):
            with open(key_path, 'r', encoding='utf-8') as f:
                self.key_data = json.load(f)
            logger.info(f"‚úÖ –ö–ª—é—á –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ —Ñ–∞–π–ª–∞ {key_path}")
        else:
            raise RuntimeError("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω –∫–ª—é—á Yandex GPT! –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ AUTHORIZED_KEY_CONTENT")
        
        self.service_account_id = self.key_data['service_account_id']
        self.private_key = self.key_data['private_key']
        self.key_id = self.key_data['id']
    
    def get_iam_token(self) -> str:
        now = time.time()
        if self.iam_token and now < self.token_expires_at:
            return self.iam_token
        
        payload = {
            'aud': "https://iam.api.cloud.yandex.net/iam/v1/tokens",
            'iss': self.service_account_id,
            'iat': int(now),
            'exp': int(now) + 3600
        }
        
        headers = {'kid': self.key_id, 'alg': 'PS256', 'typ': 'JWT'}
        encoded_token = jwt.encode(payload, self.private_key, algorithm='PS256', headers=headers)
        
        resp = requests.post(
            "https://iam.api.cloud.yandex.net/iam/v1/tokens",
            headers={"Content-Type": "application/json"},
            json={"jwt": encoded_token}
        )
        
        if resp.status_code != 200:
            raise Exception(f"Failed to get IAM token: {resp.text}")
        
        self.iam_token = resp.json()["iamToken"]
        self.token_expires_at = now + 3600
        return self.iam_token
    
    def call_gpt(self, prompt: str, max_tokens: int = 1200) -> str:
        iam_token = self.get_iam_token()
        url = "https://llm.api.cloud.yandex.net/foundationModels/v1/completion"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {iam_token}",
            "x-folder-id": self.folder_id
        }
        data = {
            "modelUri": f"gpt://{self.folder_id}/yandexgpt-lite",
            "completionOptions": {
                "stream": False,
                "temperature": 0.1,
                "maxTokens": max_tokens,
                "preset": "balanced"
            },
            "messages": [{"role": "user", "text": prompt}]
        }
        response = requests.post(url, headers=headers, json=data)
        if response.status_code != 200:
            raise Exception(f"GPT error: {response.text}")
        return response.json()['result']['alternatives'][0]['message']['text']

# ==================== DOCUMENT AGENT ====================

class DocumentAgent:
    def __init__(self, gpt_service: YandexGPTService):
        self.gpt = gpt_service
    
    def extract_text_from_pdf(self, file_content: bytes) -> str:
        try:
            from PyPDF2 import PdfReader
            reader = PdfReader(BytesIO(file_content))
            
            text = ""
            for page in reader.pages[:10]:
                extracted = page.extract_text()
                if extracted:
                    text += extracted + "\n"
                if len(text) > 5000:
                    break
            
            return text.strip()
        except Exception as e:
            logger.error(f"PDF parse error: {e}")
            return "[–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF]"
    
    def analyze_document(self, text: str) -> AnalysisResult:
        text_hash = get_text_hash(text)
        if text_hash in _analysis_cache:
            logger.info("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∑—è—Ç –∏–∑ –∫—ç—à–∞")
            return _analysis_cache[text_hash]
        
        combined_prompt = f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –∏–∑–≤–ª–µ–∫–∏ –í–°–ï –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON.

üìÑ –¢–ï–ö–°–¢ –î–û–ö–£–ú–ï–ù–¢–ê:
{text[:4000]}

üìã –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (—Å—Ç—Ä–æ–≥–æ JSON):
{{
  "extracted_data": {{
    "document_type": "contract|invoice|act|application|other",
    "document_subtype": "loan|rental|service|purchase|microloan_application|other",
    "parties": ["–°—Ç–æ—Ä–æ–Ω–∞ 1", "–°—Ç–æ—Ä–æ–Ω–∞ 2"],
    "total_amount": 5800,
    "currency": "RUB",
    "dates": {{"signature": "2024-01-01"}},
    "financial_terms": {{"interest_rate": "0.8% –≤ –¥–µ–Ω—å", "loan_term": "30 –¥–Ω–µ–π"}},
    "obligations": ["–æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ 1"],
    "penalties": "–æ–ø–∏—Å–∞–Ω–∏–µ —à—Ç—Ä–∞—Ñ–æ–≤"
  }},
  "risk_flags": [
    {{"level": "high|medium|low", "category": "financial|legal|operational", "description": "...", "suggestion": "..."}}
  ],
  "action_items": ["–¥–µ–π—Å—Ç–≤–∏–µ 1", "–¥–µ–π—Å—Ç–≤–∏–µ 2"],
  "summary": "–ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è",
  "confidence_score": 0.85
}}

‚ö†Ô∏è –ü–†–ê–í–ò–õ–ê:
‚Ä¢ parties ‚Äî –°–ü–ò–°–û–ö —Å—Ç—Ä–æ–∫
‚Ä¢ –ò–ó–í–õ–ï–ö–ê–ô –í–°–Å —á—Ç–æ –µ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ
‚Ä¢ –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äî —Å—Ç–∞–≤—å null
‚Ä¢ –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON, –±–µ–∑ markdown
‚Ä¢ confidence_score: 0.0-1.0
"""
        
        response = self.gpt.call_gpt(combined_prompt, max_tokens=1200)
        
        try:
            start = response.find('{')
            end = response.rfind('}') + 1
            data = json.loads(response[start:end])
        except Exception as e:
            logger.warning(f"JSON parse error: {e}")
            data = {
                "extracted_data": {
                    "document_type": "other",
                    "parties": [],
                    "total_amount": None,
                    "currency": "–ù–µ —É–∫–∞–∑–∞–Ω–∞",
                    "dates": {},
                    "obligations": [],
                    "penalties": None
                },
                "risk_flags": [],
                "action_items": ["–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –≤—Ä—É—á–Ω—É—é"],
                "summary": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç",
                "confidence_score": 0.3
            }
        
        ext = data.get("extracted_data", {})
        result = AnalysisResult(
            extracted_data=ExtractedData(
                document_type=DocumentType(ext.get("document_type", "other")),
                parties=ext.get("parties") or [],
                total_amount=ext.get("total_amount"),
                currency=ext.get("currency") or "–ù–µ —É–∫–∞–∑–∞–Ω–∞",
                dates=ext.get("dates") or {},
                obligations=ext.get("obligations") or [],
                penalties=ext.get("penalties")
            ),
            risk_flags=[
                RiskFlag(
                    level=RiskLevel(f.get("level", "low")),
                    category=f.get("category", "other"),
                    description=f.get("description", ""),
                    suggestion=f.get("suggestion", "")
                ) for f in (data.get("risk_flags") or [])
            ],
            action_items=data.get("action_items") or ["–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Ä—É—á–Ω—É—é"],
            summary=data.get("summary", ""),
            confidence_score=min(1.0, max(0.0, data.get("confidence_score", 0.5)))
        )
        
        _analysis_cache[text_hash] = result
        logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ –∫—ç—à (–≤—Å–µ–≥–æ: {len(_analysis_cache)})")
        
        return result

# ==================== FASTAPI APP ====================

app = FastAPI(title="DocuBot API", description="AI-–∞–≥–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", version="0.2.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Å–µ—Ä–≤–∏—Å–æ–≤
init_db()
logger.info("‚úÖ Database initialized")

FOLDER_ID = os.getenv("YANDEX_FOLDER_ID", "b1gdcuaq0il54iojm93b")
gpt_service = YandexGPTService(FOLDER_ID)
agent = DocumentAgent(gpt_service)

@app.get("/")
async def root():
    return {"message": "DocuBot API —Ä–∞–±–æ—Ç–∞–µ—Ç!", "version": "0.2.0"}

@app.get("/health")
async def health_check():
    return {"status": "ok"}

@app.get("/cache/stats")
async def cache_stats():
    return {
        "cache_size": len(_analysis_cache),
        "cache_info": get_text_hash.cache_info()
    }

@app.post("/api/analyze", response_model=DocumentUploadResponse)
async def analyze_document(file: UploadFile = File(...), db: Session = Depends(get_db)):
    logger.info(f"–ü–æ–ª—É—á–µ–Ω —Ñ–∞–π–ª: {file.filename}")
    try:
        content = await file.read()
        text = agent.extract_text_from_pdf(content)
        if not text or len(text) < 10:
            raise HTTPException(400, "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç")
        
        result = agent.analyze_document(text)
        
        try:
            history = AnalysisHistory(
                filename=file.filename,
                document_type=result.extracted_data.document_type.value,
                parties=str(result.extracted_data.parties),
                total_amount=result.extracted_data.total_amount,
                currency=result.extracted_data.currency,
                summary=result.summary,
                confidence_score=result.confidence_score,
                risk_count=len(result.risk_flags),
                full_result=result.dict(),
                user_id="web"
            )
            db.add(history)
            db.commit()
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î: {e}")
            db.rollback()
        
        return DocumentUploadResponse(status="success", result=result)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞: {str(e)}")
        return DocumentUploadResponse(status="error", error=str(e))

@app.get("/api/history")
async def get_history(limit: int = 10, skip: int = 0, db: Session = Depends(get_db)):
    try:
        analyses = db.query(AnalysisHistory).order_by(
            desc(AnalysisHistory.created_at)
        ).offset(skip).limit(limit).all()
        
        return {
            "status": "success",
            "count": len(analyses),
            "analyses": [
                {
                    "id": a.id,
                    "filename": a.filename,
                    "document_type": a.document_type,
                    "created_at": a.created_at.isoformat(),
                    "confidence_score": a.confidence_score,
                    "risk_count": a.risk_count
                }
                for a in analyses
            ]
        }
    except Exception as e:
        logger.error(f"Error fetching history: {e}")
        return {"status": "error", "error": str(e)}

@app.get("/api/stats")
async def get_stats(db: Session = Depends(get_db)):
    try:
        total_documents = db.query(AnalysisHistory).count()
        
        contracts = db.query(AnalysisHistory).filter(
            AnalysisHistory.document_type == "contract"
        ).count()
        
        invoices = db.query(AnalysisHistory).filter(
            AnalysisHistory.document_type == "invoice"
        ).count()
        
        acts = db.query(AnalysisHistory).filter(
            AnalysisHistory.document_type == "act"
        ).count()
        
        avg_confidence = db.query(
          func.avg(AnalysisHistory.confidence_score)
        ).scalar() or 0
        
        total_risks = db.query(
            func.sum(AnalysisHistory.risk_count)
        ).scalar() or 0
        
        return {
            "status": "success",
            "total_documents": total_documents,
            "by_type": {
                "contract": contracts,
                "invoice": invoices,
                "act": acts,
                "other": total_documents - contracts - invoices - acts
            },
            "avg_confidence": round(avg_confidence, 2),
            "total_risks": total_risks
        }
    except Exception as e:
        logger.error(f"Error fetching stats: {e}")
        return {"status": "error", "error": str(e)}

# ==================== PDF GENERATION ENDPOINT ====================

@app.get("/api/generate-pdf/{analysis_id}")
async def generate_pdf(analysis_id: int, db: Session = Depends(get_db)):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è PDF –æ—Ç—á—ë—Ç–∞"""
    
    analysis = db.query(AnalysisHistory).filter(
        AnalysisHistory.id == analysis_id
    ).first()
    
    if not analysis:
        raise HTTPException(404, "Analysis not found")
    
    # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º —à—Ä–∏—Ñ—Ç —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–∏—Ä–∏–ª–ª–∏—Ü—ã
    from reportlab.pdfbase import pdfmetrics
    from reportlab.pdfbase.ttfonts import TTFont
    
    try:
        # –ü—Ä–æ–±—É–µ–º –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å —à—Ä–∏—Ñ—Ç DejaVu Sans (–µ—Å—Ç—å –≤ Linux)
        pdfmetrics.registerFont(TTFont('DejaVuSans', '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf'))
        pdfmetrics.registerFont(TTFont('DejaVuSans-Bold', '/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf'))
        main_font = 'DejaVuSans'
        bold_font = 'DejaVuSans-Bold'
    except:
        # –ï—Å–ª–∏ —à—Ä–∏—Ñ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞ –Ω–µ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å)
        main_font = 'Helvetica'
        bold_font = 'Helvetica-Bold'
    
    # –°–æ–∑–¥–∞—ë–º PDF
    buffer = BytesIO()
    p = canvas.Canvas(buffer, pagesize=A4)
    width, height = A4
    y_position = height - 50
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    p.setFont(bold_font, 24)
    p.drawString(100, y_position, "DocuBot AI - Analysis Report")
    y_position -= 50
    
    # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    p.setFont(bold_font, 14)
    p.drawString(50, y_position, "üìã –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è")
    y_position -= 30
    
    p.setFont(main_font, 11)
    full_result = analysis.full_result if isinstance(analysis.full_result, dict) else json.loads(analysis.full_result)
    extracted_data = full_result.get('extracted_data', {})
    
    p.drawString(50, y_position, f"–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞: {extracted_data.get('document_type', 'N/A')}")
    y_position -= 20
    parties = extracted_data.get('parties', [])
    if isinstance(parties, list):
        parties_str = ', '.join(parties) if parties else 'N/A'
    else:
        parties_str = str(parties)
    p.drawString(50, y_position, f"–°—Ç–æ—Ä–æ–Ω—ã: {parties_str}")
    y_position -= 20
    p.drawString(50, y_position, f"–°—É–º–º–∞: {extracted_data.get('total_amount', 'N/A')} {extracted_data.get('currency', '')}")
    y_position -= 50
    
    # –†–∏—Å–∫–∏
    p.setFont(bold_font, 14)
    p.drawString(50, y_position, f"‚ö†Ô∏è –†–∏—Å–∫–∏ ({len(full_result.get('risk_flags', []))})")
    y_position -= 30
    
    for flag in full_result.get('risk_flags', []):
        p.setFont(main_font, 10)
        level = flag.get('level', '').upper()
        category = flag.get('category', '')
        description = flag.get('description', '')
        p.drawString(50, y_position, f"‚Ä¢ {level} - {category}: {description}")
        y_position -= 20
        if y_position < 100:
            p.showPage()
            y_position = height - 50
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    p.setFont(bold_font, 14)
    p.drawString(50, y_position, "‚úÖ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
    y_position -= 30
    
    for i, item in enumerate(full_result.get('action_items', []), 1):
        p.setFont(main_font, 10)
        p.drawString(50, y_position, f"{i}. {item}")
        y_position -= 20
        if y_position < 100:
            p.showPage()
            y_position = height - 50
    
    # –†–µ–∑—é–º–µ
    p.setFont(bold_font, 14)
    p.drawString(50, y_position, "üìù –†–µ–∑—é–º–µ")
    y_position -= 30
    
    p.setFont(main_font, 11)
    summary_text = full_result.get('summary', '')
    
    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å—Ç—Ä–æ–∫–∏
    words = summary_text.split()
    line = ""
    for word in words:
        if len(line) + len(word) < 70:
            line += word + " "
        else:
            p.drawString(50, y_position, line)
            y_position -= 18
            line = word + " "
            if y_position < 100:
                p.showPage()
                y_position = height - 50
    if line:
        p.drawString(50, y_position, line)
    
    p.save()
    buffer.seek(0)
    
    return StreamingResponse(
        buffer,
        media_type="application/pdf",
        headers={"Content-Disposition": f"attachment; filename=docubot-analysis-{analysis_id}.pdf"}
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=int(os.getenv("PORT", 10000)))