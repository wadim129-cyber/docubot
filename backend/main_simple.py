# backend/main_simple.py
import sys
import os
import json
import time
import jwt
import requests
import logging
import hashlib
from io import BytesIO
from datetime import datetime
from typing import List, Optional, Dict
from enum import Enum
from functools import lru_cache

from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∂–∞–µ–º .env
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ==================== –ö–≠–®–ò–†–û–í–ê–ù–ò–ï ====================

@lru_cache(maxsize=50)
def get_text_hash(text: str) -> str:
    """–•—ç—à –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    return hashlib.md5(text[:2000].encode()).hexdigest()

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
_analysis_cache: Dict[str, 'AnalysisResult'] = {}

# ==================== –ú–û–î–ï–õ–ò ====================

class DocumentType(str, Enum):
    CONTRACT = "contract"
    INVOICE = "invoice"
    ACT = "act"
    OTHER = "other"

class RiskLevel(str, Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"

class RiskFlag(BaseModel):
    level: RiskLevel
    category: str
    description: str
    suggestion: str

class ExtractedData(BaseModel):
    document_type: DocumentType
    parties: List[str] = Field(default_factory=list)
    total_amount: Optional[float] = None
    currency: Optional[str] = "RUB"
    dates: Dict[str, Optional[str]] = Field(default_factory=dict)
    obligations: List[str] = Field(default_factory=list)
    penalties: Optional[str] = None

class AnalysisResult(BaseModel):
    extracted_data: ExtractedData
    risk_flags: List[RiskFlag] = Field(default_factory=list)
    action_items: List[str] = Field(default_factory=list)
    summary: str
    confidence_score: float = Field(ge=0, le=1)

class DocumentUploadResponse(BaseModel):
    status: str
    result: Optional[AnalysisResult] = None
    error: Optional[str] = None

# ==================== YANDEX GPT SERVICE ====================

class YandexGPTService:
    def __init__(self, folder_id: str, key_path: str):
        self.folder_id = folder_id
        self.iam_token = None
        self.token_expires_at = 0
        
        # üîë –ß–∏—Ç–∞–µ–º –∫–ª—é—á –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è –ò–õ–ò –∏–∑ —Ñ–∞–π–ª–∞
        key_content = os.getenv('AUTHORIZED_KEY_CONTENT')
        if key_content:
            self.key_data = json.loads(key_content)
            logger.info("‚úÖ –ö–ª—é—á –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è")
        else:
            logger.info(f"üìÅ –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–ª—é—á –∏–∑ —Ñ–∞–π–ª–∞: {key_path}")
            with open(key_path, 'r', encoding='utf-8') as f:
                self.key_data = json.load(f)
        
        self.service_account_id = self.key_data['service_account_id']
        self.private_key = self.key_data['private_key']
        self.key_id = self.key_data['id']
    
    def get_iam_token(self) -> str:
        now = time.time()
        if self.iam_token and now < self.token_expires_at:
            return self.iam_token
        
        payload = {
            'aud': "https://iam.api.cloud.yandex.net/iam/v1/tokens",
            'iss': self.service_account_id,
            'iat': int(now),
            'exp': int(now) + 3600
        }
        
        headers = {'kid': self.key_id, 'alg': 'PS256', 'typ': 'JWT'}
        encoded_token = jwt.encode(payload, self.private_key, algorithm='PS256', headers=headers)
        
        resp = requests.post(
            "https://iam.api.cloud.yandex.net/iam/v1/tokens",
            headers={"Content-Type": "application/json"},
            json={"jwt": encoded_token}
        )
        
        if resp.status_code != 200:
            raise Exception(f"Failed to get IAM token: {resp.text}")
        
        self.iam_token = resp.json()["iamToken"]
        self.token_expires_at = now + 3600
        return self.iam_token
    
    def call_gpt(self, prompt: str, max_tokens: int = 1200) -> str:
        iam_token = self.get_iam_token()
        url = "https://llm.api.cloud.yandex.net/foundationModels/v1/completion"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {iam_token}",
            "x-folder-id": self.folder_id
        }
        data = {
            "modelUri": f"gpt://{self.folder_id}/yandexgpt-lite",
            "completionOptions": {
                "stream": False,
                "temperature": 0.1,
                "maxTokens": max_tokens,
                "preset": "balanced"
            },
            "messages": [{"role": "user", "text": prompt}]
        }
        response = requests.post(url, headers=headers, json=data)
        if response.status_code != 200:
            raise Exception(f"GPT error: {response.text}")
        return response.json()['result']['alternatives'][0]['message']['text']

# ==================== DOCUMENT AGENT ====================

class DocumentAgent:
    def __init__(self, gpt_service: YandexGPTService):
        self.gpt = gpt_service
    
    def extract_text_from_pdf(self, file_content: bytes) -> str:
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º"""
        try:
            from PyPDF2 import PdfReader
            reader = PdfReader(BytesIO(file_content))
            
            # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–∞–Ω–∏—Ü (—ç–∫–æ–Ω–æ–º–∏—è –≤—Ä–µ–º–µ–Ω–∏)
            text = ""
            for page in reader.pages[:10]:
                extracted = page.extract_text()
                if extracted:
                    text += extracted + "\n"
                if len(text) > 5000:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—ä—ë–º
                    break
            
            return text.strip()
        except Exception as e:
            logger.error(f"PDF parse error: {e}")
            return "[–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF]"
    
    def analyze_document(self, text: str) -> AnalysisResult:
        # üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        text_hash = get_text_hash(text)
        if text_hash in _analysis_cache:
            logger.info("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∑—è—Ç –∏–∑ –∫—ç—à–∞")
            return _analysis_cache[text_hash]
        
        # üî• –û–î–ò–ù –∑–∞–ø—Ä–æ—Å –≤–º–µ—Å—Ç–æ —á–µ—Ç—ã—Ä—ë—Ö!
        combined_prompt = f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –∏ –≤–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON:

üìÑ –¢–ï–ö–°–¢ –î–û–ö–£–ú–ï–ù–¢–ê:
{text[:4000]}

üìã –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (—Å—Ç—Ä–æ–≥–æ JSON, –±–µ–∑ markdown):
{{
  "extracted_data": {{
    "document_type": "contract|invoice|act|other",
    "parties": ["–°—Ç–æ—Ä–æ–Ω–∞ 1", "–°—Ç–æ—Ä–æ–Ω–∞ 2"],
    "total_amount": 100000 –∏–ª–∏ null,
    "currency": "RUB|USD|EUR" –∏–ª–∏ null,
    "dates": {{"signature": "2024-01-01" –∏–ª–∏ null}},
    "obligations": ["–æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ 1"],
    "penalties": "–æ–ø–∏—Å–∞–Ω–∏–µ —à—Ç—Ä–∞—Ñ–æ–≤" –∏–ª–∏ null
  }},
  "risk_flags": [
    {{"level": "high|medium|low", "category": "financial|legal|operational", "description": "...", "suggestion": "..."}}
  ],
  "action_items": ["–¥–µ–π—Å—Ç–≤–∏–µ 1", "–¥–µ–π—Å—Ç–≤–∏–µ 2", "–¥–µ–π—Å—Ç–≤–∏–µ 3"],
  "summary": "–ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è",
  "confidence_score": 0.85
}}

‚ö†Ô∏è –ü–†–ê–í–ò–õ–ê:
‚Ä¢ –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äî —Å—Ç–∞–≤—å null, –Ω–µ –≤—ã–¥—É–º—ã–≤–∞–π
‚Ä¢ currency: –≤—Å–µ–≥–¥–∞ —Å—Ç—Ä–æ–∫–∞, –¥–∞–∂–µ –µ—Å–ª–∏ null ‚Üí "–ù–µ —É–∫–∞–∑–∞–Ω–∞"
‚Ä¢ dates.signature: —Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ null
‚Ä¢ confidence_score: 0.0-1.0
‚Ä¢ –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π
"""
        
        response = self.gpt.call_gpt(combined_prompt, max_tokens=1200)
        
        # –ü–∞—Ä—Å–∏–º JSON
        try:
            start = response.find('{')
            end = response.rfind('}') + 1
            data = json.loads(response[start:end])
        except Exception as e:
            logger.warning(f"JSON parse error: {e}")
            data = {
                "extracted_data": {
                    "document_type": "other",
                    "parties": [],
                    "total_amount": None,
                    "currency": "–ù–µ —É–∫–∞–∑–∞–Ω–∞",
                    "dates": {},
                    "obligations": [],
                    "penalties": None
                },
                "risk_flags": [],
                "action_items": ["–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –≤—Ä—É—á–Ω—É—é"],
                "summary": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç",
                "confidence_score": 0.3
            }
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ Pydantic –º–æ–¥–µ–ª–∏
        ext = data.get("extracted_data", {})
        result = AnalysisResult(
            extracted_data=ExtractedData(
                document_type=DocumentType(ext.get("document_type", "other")),
                parties=ext.get("parties") or [],
                total_amount=ext.get("total_amount"),
                currency=ext.get("currency") or "–ù–µ —É–∫–∞–∑–∞–Ω–∞",
                dates=ext.get("dates") or {},
                obligations=ext.get("obligations") or [],
                penalties=ext.get("penalties")
            ),
            risk_flags=[
                RiskFlag(
                    level=RiskLevel(f.get("level", "low")),
                    category=f.get("category", "other"),
                    description=f.get("description", ""),
                    suggestion=f.get("suggestion", "")
                ) for f in (data.get("risk_flags") or [])
            ],
            action_items=data.get("action_items") or ["–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Ä—É—á–Ω—É—é"],
            summary=data.get("summary", ""),
            confidence_score=min(1.0, max(0.0, data.get("confidence_score", 0.5)))
        )
        
        # üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        _analysis_cache[text_hash] = result
        logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ –∫—ç—à (–≤—Å–µ–≥–æ: {len(_analysis_cache)})")
        
        return result

# ==================== FASTAPI APP ====================

app = FastAPI(title="DocuBot API", description="AI-–∞–≥–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", version="0.2.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤
FOLDER_ID = os.getenv("YANDEX_FOLDER_ID", "b1gdcuaq0il54iojm93b")
KEY_PATH = os.path.join(os.path.dirname(__file__), "../authorized_key.json")
gpt_service = YandexGPTService(FOLDER_ID, KEY_PATH)
agent = DocumentAgent(gpt_service)

@app.get("/")
async def root():
    return {"message": "DocuBot API —Ä–∞–±–æ—Ç–∞–µ—Ç!", "version": "0.2.0"}

@app.get("/health")
async def health_check():
    return {"status": "ok"}

@app.get("/cache/stats")
async def cache_stats():
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏"""
    return {
        "cache_size": len(_analysis_cache),
        "cache_info": get_text_hash.cache_info()
    }

@app.post("/api/analyze", response_model=DocumentUploadResponse)
async def analyze_document(file: UploadFile = File(...)):
    logger.info(f"–ü–æ–ª—É—á–µ–Ω —Ñ–∞–π–ª: {file.filename}")
    try:
        content = await file.read()
        text = agent.extract_text_from_pdf(content)
        if not text or len(text) < 10:
            raise HTTPException(400, "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç")
        result = agent.analyze_document(text)
        return DocumentUploadResponse(status="success", result=result)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞: {str(e)}")
        return DocumentUploadResponse(status="error", error=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=int(os.getenv("PORT", 10000)))